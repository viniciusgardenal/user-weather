'use strict';
const fs = require('fs');
const csv = require('csv-parser');
const path = require('path');

module.exports = {
  async up(queryInterface, Sequelize) {
    const filePath = path.join(__dirname, '..', 'users.csv'); 
    const usersToInsert = [];
    const limit = 100; // Nosso novo limite

    console.log(`Iniciando o processo para inserir os primeiros ${limit} usu√°rios...`);

    const stream = fs.createReadStream(filePath).pipe(csv());

    // Criamos uma promessa para saber quando a leitura terminar
    const streamPromise = new Promise((resolve, reject) => {
      stream.on('data', (row) => {
        // Adicionamos usu√°rios ao array APENAS se ainda n√£o atingimos o limite
        if (usersToInsert.length < limit) {
          usersToInsert.push({
            id: row.id,
            name: row.name,
            email: row.email,
            createdAt: new Date(),
            updatedAt: new Date(),
          });
        } else {
          // Se j√° temos 100 usu√°rios, paramos de ler o arquivo.
          // stream.destroy() encerra a leitura imediatamente.
          stream.destroy();
          resolve();
        }
      });

      stream.on('end', () => {
        // O evento 'end' √© chamado quando o arquivo termina ou quando a stream √© destru√≠da.
        console.log('Leitura do arquivo finalizada.');
        resolve();
      });

      stream.on('error', (err) => {
        reject(err);
      });
    });

    // Espera a leitura do arquivo terminar
    await streamPromise;

    // Insere o lote √∫nico de 100 usu√°rios no banco de dados
    if (usersToInsert.length > 0) {
      await queryInterface.bulkInsert('Users', usersToInsert, {});
      console.log(`${usersToInsert.length} usu√°rios foram inseridos com sucesso.`);
    }

    console.log('PROCESSO DE SEED CONCLU√çDO! üéâ');
  },

  async down(queryInterface, Sequelize) {
    await queryInterface.bulkDelete('Users', null, {});
  },
};